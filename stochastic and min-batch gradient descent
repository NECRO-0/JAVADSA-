import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the independent variable
url_x = "https://drive.google.com/file/d/1cFZHElm5ebn1OolgmrLOxw91T8am3S5C/view?usp=sharing"
file_id_x = url_x.split("/")[-2]
download_link_x = f"https://drive.google.com/uc?id={file_id_x}"
x_data = pd.read_csv(download_link_x)

# Load the dependent variable
url_y = "https://drive.google.com/file/d/1rY3oTHxa1FT3fdcjWuKKQ4DOXnU8AeQM/view?usp=sharing"
file_id_y = url_y.split("/")[-2]
download_link_y = f"https://drive.google.com/uc?id={file_id_y}"
y_data = pd.read_csv(download_link_y)

# Initialize parameters
theta_0 = 0
theta_1 = 0

# Set learning rate and convergence criterion for batch gradient descent
learning_rate_batch = 0.5
convergence_criterion_batch = 0.0001
max_iterations_batch = 1000

# Set learning rate and convergence criterion for stochastic gradient descent
learning_rate_stochastic = 0.01
convergence_criterion_stochastic = 0.0001
max_iterations_stochastic = 1000

# Set learning rate and convergence criterion for mini-batch gradient descent
learning_rate_mini_batch = 0.1
convergence_criterion_mini_batch = 0.0001
max_iterations_mini_batch = 1000
batch_size = 32  # You can adjust the batch size

# Number of training examples
m = len(x_data)

# Function to perform gradient descent
def gradient_descent(x, y, theta_0, theta_1, learning_rate, max_iterations, convergence_criterion):
    iteration_list = []
    cost_history = []
    prev_cost = float('inf')

    for iteration in range(max_iterations):
        predictions = hypothesis(theta_0, theta_1, x)
        errors = predictions - y

        gradient_theta_0 = (1 / m) * np.sum(errors)
        gradient_theta_1 = (1 / m) * np.sum(errors * x)

        theta_0 = theta_0 - learning_rate * gradient_theta_0
        theta_1 = theta_1 - learning_rate * gradient_theta_1

        current_cost = cost_function(theta_0, theta_1, x, y)

        iteration_list.append(iteration)
        cost_history.append(current_cost)

        if np.abs(prev_cost - current_cost).any() < convergence_criterion:
            print(f"Converged after {iteration + 1} iterations.")
            break

        prev_cost = current_cost

    return iteration_list, cost_history

# Run batch gradient descent
iteration_list_batch, cost_history_batch = gradient_descent(x_data, y_data, theta_0, theta_1,
                                                            learning_rate_batch, max_iterations_batch,
                                                            convergence_criterion_batch)

# Run stochastic gradient descent
iteration_list_stochastic, cost_history_stochastic = gradient_descent(x_data, y_data, theta_0, theta_1,
                                                                      learning_rate_stochastic, max_iterations_stochastic,
                                                                      convergence_criterion_stochastic)

# Run mini-batch gradient descent
iteration_list_mini_batch, cost_history_mini_batch = gradient_descent(x_data, y_data, theta_0, theta_1,
                                                                      learning_rate_mini_batch, max_iterations_mini_batch,
                                                                      convergence_criterion_mini_batch)

# Plot cost function versus iteration for each gradient descent type
plt.plot(iteration_list_batch, cost_history_batch, label='Batch GD')
plt.plot(iteration_list_stochastic, cost_history_stochastic, label='Stochastic GD')
plt.plot(iteration_list_mini_batch, cost_history_mini_batch, label='Mini-Batch GD')
plt.title('Cost Function vs. Iteration for Different Gradient Descent Types')
plt.xlabel('Iteration')
plt.ylabel('Cost Function')
plt.legend()
plt.show()
